{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Design for cfDNA Cancer Detection\n",
    "\n",
    "This notebook explores how configuration choices affect statistical power and signal-to-noise ratio in cfDNA epigenomic cancer detection studies.\n",
    "\n",
    "## Key Questions:\n",
    "1. How does sample size affect detection power?\n",
    "2. What are the effects of batch size and imbalance?\n",
    "3. How do effect sizes impact statistical significance?\n",
    "4. What are common design pitfalls to avoid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add src to path to import our modules\n",
    "sys.path.append('../src')\n",
    "\n",
    "from cfdna.simulate import simulate_dataset\n",
    "from cfdna.features import prepare_features\n",
    "from cfdna.metrics import auroc_with_ci\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sample Size Power Analysis\n",
    "\n",
    "Let's examine how sample size affects our ability to detect cancer signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base configuration\n",
    "base_config = {\n",
    "    \"dataset\": {\n",
    "        \"name\": \"power_analysis\",\n",
    "        \"random_seed\": 42\n",
    "    },\n",
    "    \"samples\": {\n",
    "        \"n_batches\": 3,\n",
    "        \"centers\": [\"site_a\", \"site_b\", \"site_c\"],\n",
    "        \"age_range\": [40, 80],\n",
    "        \"sex_ratio\": 0.5\n",
    "    },\n",
    "    \"methylation\": {\n",
    "        \"n_cpgs\": 5000,\n",
    "        \"n_dmrs\": 50,\n",
    "        \"cpgs_per_dmr\": 100,\n",
    "        \"dmr_effect_size_mean\": 0.15,\n",
    "        \"dmr_effect_size_std\": 0.05,\n",
    "        \"alpha_base\": 2.0,\n",
    "        \"beta_base\": 8.0,\n",
    "        \"batch_effect_std\": 0.02,\n",
    "        \"missingness_rate\": 0.05\n",
    "    },\n",
    "    \"fragmentomics\": {\n",
    "        \"size_bins\": [50, 100, 150, 200, 250, 300, 400, 500],\n",
    "        \"tss_enrichment_bins\": 10,\n",
    "        \"size_effect_mean\": 0.1,\n",
    "        \"size_effect_std\": 0.03,\n",
    "        \"tss_effect_mean\": 0.08,\n",
    "        \"tss_effect_std\": 0.02,\n",
    "        \"noise_std\": 0.05\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Base configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size analysis\n",
    "sample_sizes = [100, 200, 400, 600, 800, 1000]\n",
    "results = []\n",
    "\n",
    "for n_total in sample_sizes:\n",
    "    print(f\"Testing sample size: {n_total}\")\n",
    "    \n",
    "    # Update config\n",
    "    config = base_config.copy()\n",
    "    config[\"dataset\"].update({\n",
    "        \"n_samples\": n_total,\n",
    "        \"n_controls\": int(n_total * 0.6),\n",
    "        \"n_cancer\": int(n_total * 0.4)\n",
    "    })\n",
    "    \n",
    "    # Run multiple replicates\n",
    "    aurocs = []\n",
    "    for rep in range(5):  # 5 replicates\n",
    "        config[\"dataset\"][\"random_seed\"] = 42 + rep\n",
    "        \n",
    "        # Save config temporarily\n",
    "        config_path = Path(f\"temp_config_{n_total}_{rep}.yaml\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            yaml.dump(config, f)\n",
    "        \n",
    "        try:\n",
    "            # Generate data\n",
    "            data_dir = Path(\"temp_data\")\n",
    "            data_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            stats = simulate_dataset(config_path, data_dir)\n",
    "            \n",
    "            # Prepare features and train simple model\n",
    "            data = prepare_features(data_dir, config)\n",
    "            \n",
    "            # Quick logistic regression for power estimate\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            from sklearn.metrics import roc_auc_score\n",
    "            \n",
    "            X = data[\"X\"]\n",
    "            y = data[\"y\"]\n",
    "            splits = data[\"splits\"]\n",
    "            \n",
    "            X_train = X.iloc[splits[\"train\"]]\n",
    "            y_train = y.iloc[splits[\"train\"]]\n",
    "            X_test = X.iloc[splits[\"test\"]]\n",
    "            y_test = y.iloc[splits[\"test\"]]\n",
    "            \n",
    "            # Standardize features\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train.fillna(X_train.median()))\n",
    "            X_test_scaled = scaler.transform(X_test.fillna(X_train.median()))\n",
    "            \n",
    "            # Train model\n",
    "            model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_probs = model.predict_proba(X_test_scaled)[:, 1]\n",
    "            auroc = roc_auc_score(y_test, y_probs)\n",
    "            aurocs.append(auroc)\n",
    "            \n",
    "            # Cleanup\n",
    "            import shutil\n",
    "            shutil.rmtree(data_dir, ignore_errors=True)\n",
    "            config_path.unlink()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with sample size {n_total}, rep {rep}: {e}\")\n",
    "            aurocs.append(np.nan)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'sample_size': n_total,\n",
    "        'mean_auroc': np.nanmean(aurocs),\n",
    "        'std_auroc': np.nanstd(aurocs),\n",
    "        'min_auroc': np.nanmin(aurocs),\n",
    "        'max_auroc': np.nanmax(aurocs)\n",
    "    })\n",
    "\n",
    "power_df = pd.DataFrame(results)\n",
    "print(\"Power analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot power analysis results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# AUROC vs sample size\n",
    "ax1.errorbar(power_df['sample_size'], power_df['mean_auroc'], \n",
    "             yerr=power_df['std_auroc'], marker='o', capsize=5)\n",
    "ax1.fill_between(power_df['sample_size'], power_df['min_auroc'], \n",
    "                power_df['max_auroc'], alpha=0.3)\n",
    "ax1.set_xlabel('Sample Size')\n",
    "ax1.set_ylabel('AUROC')\n",
    "ax1.set_title('Detection Power vs Sample Size')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0.5, 1.0)\n",
    "\n",
    "# Variability vs sample size\n",
    "ax2.plot(power_df['sample_size'], power_df['std_auroc'], marker='s', color='red')\n",
    "ax2.set_xlabel('Sample Size')\n",
    "ax2.set_ylabel('AUROC Standard Deviation')\n",
    "ax2.set_title('Performance Stability vs Sample Size')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nPower Analysis Summary:\")\n",
    "print(power_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Effect Size Analysis\n",
    "\n",
    "How do different effect sizes impact detectability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect size analysis\n",
    "effect_sizes = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30]\n",
    "effect_results = []\n",
    "\n",
    "for effect_size in effect_sizes:\n",
    "    print(f\"Testing effect size: {effect_size}\")\n",
    "    \n",
    "    config = base_config.copy()\n",
    "    config[\"dataset\"].update({\n",
    "        \"n_samples\": 400,\n",
    "        \"n_controls\": 240,\n",
    "        \"n_cancer\": 160\n",
    "    })\n",
    "    \n",
    "    # Update effect sizes\n",
    "    config[\"methylation\"][\"dmr_effect_size_mean\"] = effect_size\n",
    "    config[\"fragmentomics\"][\"size_effect_mean\"] = effect_size * 0.67\n",
    "    config[\"fragmentomics\"][\"tss_effect_mean\"] = effect_size * 0.53\n",
    "    \n",
    "    aurocs = []\n",
    "    for rep in range(3):  # Fewer reps for speed\n",
    "        config[\"dataset\"][\"random_seed\"] = 42 + rep\n",
    "        \n",
    "        # Save and run\n",
    "        config_path = Path(f\"temp_effect_{effect_size}_{rep}.yaml\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            yaml.dump(config, f)\n",
    "        \n",
    "        try:\n",
    "            data_dir = Path(\"temp_data\")\n",
    "            data_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            simulate_dataset(config_path, data_dir)\n",
    "            data = prepare_features(data_dir, config)\n",
    "            \n",
    "            # Quick evaluation\n",
    "            X = data[\"X\"]\n",
    "            y = data[\"y\"]\n",
    "            splits = data[\"splits\"]\n",
    "            \n",
    "            X_train = X.iloc[splits[\"train\"]]\n",
    "            y_train = y.iloc[splits[\"train\"]]\n",
    "            X_test = X.iloc[splits[\"test\"]]\n",
    "            y_test = y.iloc[splits[\"test\"]]\n",
    "            \n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            from sklearn.metrics import roc_auc_score\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train.fillna(X_train.median()))\n",
    "            X_test_scaled = scaler.transform(X_test.fillna(X_train.median()))\n",
    "            \n",
    "            model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            y_probs = model.predict_proba(X_test_scaled)[:, 1]\n",
    "            auroc = roc_auc_score(y_test, y_probs)\n",
    "            aurocs.append(auroc)\n",
    "            \n",
    "            # Cleanup\n",
    "            import shutil\n",
    "            shutil.rmtree(data_dir, ignore_errors=True)\n",
    "            config_path.unlink()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with effect size {effect_size}, rep {rep}: {e}\")\n",
    "            aurocs.append(np.nan)\n",
    "    \n",
    "    effect_results.append({\n",
    "        'effect_size': effect_size,\n",
    "        'mean_auroc': np.nanmean(aurocs),\n",
    "        'std_auroc': np.nanstd(aurocs)\n",
    "    })\n",
    "\n",
    "effect_df = pd.DataFrame(effect_results)\n",
    "print(\"Effect size analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot effect size results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(effect_df['effect_size'], effect_df['mean_auroc'], \n",
    "             yerr=effect_df['std_auroc'], marker='o', capsize=5, linewidth=2)\n",
    "plt.xlabel('DMR Effect Size')\n",
    "plt.ylabel('AUROC')\n",
    "plt.title('Detection Performance vs Effect Size')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0.5, 1.0)\n",
    "\n",
    "# Add reference lines\n",
    "plt.axhline(y=0.7, color='orange', linestyle='--', alpha=0.7, label='Weak Signal (0.7)')\n",
    "plt.axhline(y=0.8, color='green', linestyle='--', alpha=0.7, label='Strong Signal (0.8)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEffect Size Analysis Summary:\")\n",
    "print(effect_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Common Design Pitfalls\n",
    "\n",
    "### Pitfall 1: Batch Confounding\n",
    "When cancer samples are concentrated in specific batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate batch confounding\n",
    "print(\"Demonstrating batch confounding effects...\")\n",
    "\n",
    "# Simulate a confounded design where batch 3 has more cancer samples\n",
    "# This is BAD practice but demonstrates the issue\n",
    "\n",
    "# Create manually confounded metadata\n",
    "np.random.seed(42)\n",
    "n_samples = 300\n",
    "\n",
    "# Batch assignment (confounded)\n",
    "batch_labels = np.concatenate([\n",
    "    np.full(100, 0),  # Batch 0: mostly controls\n",
    "    np.full(100, 1),  # Batch 1: mixed\n",
    "    np.full(100, 2)   # Batch 2: mostly cancer\n",
    "])\n",
    "\n",
    "# Cancer labels (confounded with batch)\n",
    "cancer_probs = np.array([0.2, 0.5, 0.8])  # Different cancer rates per batch\n",
    "cancer_labels = []\n",
    "for i in range(n_samples):\n",
    "    batch = batch_labels[i]\n",
    "    is_cancer = np.random.random() < cancer_probs[batch]\n",
    "    cancer_labels.append(int(is_cancer))\n",
    "\n",
    "cancer_labels = np.array(cancer_labels)\n",
    "\n",
    "# Create mock \"methylation\" data that's just batch effects\n",
    "batch_effects = np.array([0.3, 0.5, 0.7])  # Different means per batch\n",
    "mock_methylation = []\n",
    "for i in range(n_samples):\n",
    "    batch = batch_labels[i]\n",
    "    # Data is just batch effect + noise\n",
    "    features = np.random.normal(batch_effects[batch], 0.1, 50)\n",
    "    mock_methylation.append(features)\n",
    "\n",
    "mock_methylation = np.array(mock_methylation)\n",
    "\n",
    "# Train model on confounded data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test, batch_train, batch_test = train_test_split(\n",
    "    mock_methylation, cancer_labels, batch_labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "model_confounded = LogisticRegression(random_state=42)\n",
    "model_confounded.fit(X_train, y_train)\n",
    "\n",
    "y_probs_confounded = model_confounded.predict_proba(X_test)[:, 1]\n",
    "auroc_confounded = roc_auc_score(y_test, y_probs_confounded)\n",
    "\n",
    "print(f\"AUROC with batch confounding: {auroc_confounded:.3f}\")\n",
    "print(\"This high AUROC is misleading - it's detecting batch effects, not cancer!\")\n",
    "\n",
    "# Show batch distribution\n",
    "batch_df = pd.DataFrame({\n",
    "    'batch': batch_labels,\n",
    "    'cancer': cancer_labels\n",
    "})\n",
    "\n",
    "print(\"\\nBatch-Cancer Distribution (PROBLEMATIC):\")\n",
    "print(pd.crosstab(batch_df['batch'], batch_df['cancer'], normalize='index').round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitfall 2: Data Leakage in Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate data leakage\n",
    "print(\"Demonstrating preprocessing data leakage...\")\n",
    "\n",
    "# Generate toy data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "n_features = 100\n",
    "\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "y = np.random.choice([0, 1], n_samples)\n",
    "\n",
    "# Add a weak signal\n",
    "signal_features = [0, 1, 2]\n",
    "for feature_idx in signal_features:\n",
    "    X[y == 1, feature_idx] += 0.3  # Small effect\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# WRONG: Scale using all data (leakage)\n",
    "scaler_leaky = StandardScaler()\n",
    "X_all_scaled = scaler_leaky.fit_transform(X)  # Using ALL data!\n",
    "X_train_leaky = X_all_scaled[:len(X_train)]\n",
    "X_test_leaky = X_all_scaled[len(X_train):]\n",
    "\n",
    "model_leaky = LogisticRegression(random_state=42)\n",
    "model_leaky.fit(X_train_leaky, y_train)\n",
    "auroc_leaky = roc_auc_score(y_test, model_leaky.predict_proba(X_test_leaky)[:, 1])\n",
    "\n",
    "# CORRECT: Scale using only training data\n",
    "scaler_correct = StandardScaler()\n",
    "X_train_correct = scaler_correct.fit_transform(X_train)  # Fit only on training!\n",
    "X_test_correct = scaler_correct.transform(X_test)       # Transform test\n",
    "\n",
    "model_correct = LogisticRegression(random_state=42)\n",
    "model_correct.fit(X_train_correct, y_train)\n",
    "auroc_correct = roc_auc_score(y_test, model_correct.predict_proba(X_test_correct)[:, 1])\n",
    "\n",
    "print(f\"AUROC with data leakage: {auroc_leaky:.3f}\")\n",
    "print(f\"AUROC without leakage: {auroc_correct:.3f}\")\n",
    "print(f\"Inflation due to leakage: {auroc_leaky - auroc_correct:.3f}\")\n",
    "print(\"\\nLeakage often leads to overly optimistic performance estimates!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary and Recommendations\n",
    "\n",
    "### Key Findings:\n",
    "1. **Sample Size**: Performance improves and stabilizes with larger sample sizes\n",
    "2. **Effect Size**: Larger biological effects lead to better detection performance\n",
    "3. **Batch Effects**: Can create spurious signals if confounded with outcome\n",
    "4. **Data Leakage**: Can inflate performance estimates substantially\n",
    "\n",
    "### Design Recommendations:\n",
    "1. **Randomize batch assignment** across cancer status\n",
    "2. **Use appropriate sample sizes** based on expected effect sizes\n",
    "3. **Implement strict train/validation/test splits** before any preprocessing\n",
    "4. **Include batch correction** in your analysis pipeline\n",
    "5. **Report confidence intervals** to assess uncertainty\n",
    "6. **Validate findings** in independent cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Sample size effect\n",
    "ax1.plot(power_df['sample_size'], power_df['mean_auroc'], 'o-', linewidth=2)\n",
    "ax1.set_xlabel('Sample Size')\n",
    "ax1.set_ylabel('AUROC')\n",
    "ax1.set_title('A. Sample Size vs Performance')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Effect size impact\n",
    "ax2.plot(effect_df['effect_size'], effect_df['mean_auroc'], 's-', color='orange', linewidth=2)\n",
    "ax2.set_xlabel('Effect Size')\n",
    "ax2.set_ylabel('AUROC')\n",
    "ax2.set_title('B. Effect Size vs Performance')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Batch confounding illustration\n",
    "batches = [0, 1, 2]\n",
    "cancer_rates = [0.2, 0.5, 0.8]\n",
    "ax3.bar(batches, cancer_rates, color=['lightblue', 'orange', 'red'], alpha=0.7)\n",
    "ax3.set_xlabel('Batch')\n",
    "ax3.set_ylabel('Cancer Rate')\n",
    "ax3.set_title('C. Problematic Batch Design')\n",
    "ax3.set_ylim(0, 1)\n",
    "\n",
    "# Data leakage impact\n",
    "methods = ['With Leakage', 'Without Leakage']\n",
    "aurocs = [auroc_leaky, auroc_correct]\n",
    "colors = ['red', 'green']\n",
    "bars = ax4.bar(methods, aurocs, color=colors, alpha=0.7)\n",
    "ax4.set_ylabel('AUROC')\n",
    "ax4.set_title('D. Data Leakage Impact')\n",
    "ax4.set_ylim(0.4, 0.8)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, auroc in zip(bars, aurocs):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{auroc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.suptitle('Experimental Design Considerations for cfDNA Studies', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENTAL DESIGN CHECKLIST:\")\n",
    "print(\"=\"*60)\n",
    "print(\"✓ Randomize sample collection across batches\")\n",
    "print(\"✓ Balance cancer/control samples within batches\")\n",
    "print(\"✓ Define train/test splits BEFORE preprocessing\")\n",
    "print(\"✓ Include batch correction in analysis\")\n",
    "print(\"✓ Use appropriate sample sizes for expected effects\")\n",
    "print(\"✓ Report confidence intervals\")\n",
    "print(\"✓ Validate in independent cohorts\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: Baselines vs MLP\n",
    "\n",
    "This notebook demonstrates training and comparing baseline models with multi-layer perceptron (MLP) for cfDNA cancer detection.\n",
    "\n",
    "## Objectives:\n",
    "1. Train baseline models (Logistic Regression, Random Forest)\n",
    "2. Train MLP model with hyperparameter tuning\n",
    "3. Compare model performance\n",
    "4. Analyze results and generate visualizations\n",
    "5. Demonstrate nested cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from cfdna.metrics import comprehensive_evaluation, compare_models, delong_test\n",
    "from cfdna.viz import plot_roc_curve, plot_precision_recall_curve\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train Models Using CLI\n",
    "\n",
    "We'll use the command-line interface to train different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create artifacts directory\n",
    "artifacts_dir = Path('../artifacts')\n",
    "artifacts_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# List of models to train\n",
    "models_to_train = ['logistic_l1', 'logistic_l2', 'random_forest', 'mlp']\n",
    "\n",
    "print(\"Training models using CLI...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "training_results = {}\n",
    "\n",
    "for model_name in models_to_train:\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    # Run training command\n",
    "    cmd = [\n",
    "        'python', '-m', 'cfdna.train',\n",
    "        '../data/synthetic_config.yaml',\n",
    "        model_name,\n",
    "        str(artifacts_dir),\n",
    "        '--seed', '42'\n",
    "    ]\n",
    "    \n",
    "    if model_name in ['logistic_l1', 'logistic_l2']:  # Add calibration for baselines\n",
    "        cmd.append('--calibrate')\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, cwd='../src')\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"  ✓ {model_name} training completed\")\n",
    "        # Extract AUROC from output\n",
    "        lines = result.stdout.split('\\n')\n",
    "        for line in lines:\n",
    "            if 'Test AUROC:' in line:\n",
    "                auroc_text = line.split('Test AUROC:')[1].strip()\n",
    "                auroc_value = float(auroc_text.split()[0])\n",
    "                training_results[model_name] = auroc_value\n",
    "                print(f\"    Test AUROC: {auroc_value:.3f}\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"  ✗ {model_name} training failed:\")\n",
    "        print(f\"    {result.stderr}\")\n",
    "        training_results[model_name] = None\n",
    "\n",
    "print(\"\\nTraining Summary:\")\n",
    "for model, auroc in training_results.items():\n",
    "    if auroc is not None:\n",
    "        print(f\"  {model}: {auroc:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {model}: Failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Compare Results\n",
    "\n",
    "Load the saved model results and create comprehensive comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model results\n",
    "print(\"Loading model results...\")\n",
    "\n",
    "model_results = {}\n",
    "model_predictions = {}\n",
    "\n",
    "for model_name in models_to_train:\n",
    "    results_file = artifacts_dir / f\"{model_name}_results.json\"\n",
    "    predictions_file = artifacts_dir / f\"{model_name}_test_predictions.csv\"\n",
    "    \n",
    "    if results_file.exists():\n",
    "        with open(results_file) as f:\n",
    "            model_results[model_name] = json.load(f)\n",
    "        print(f\"  ✓ Loaded {model_name} results\")\n",
    "    else:\n",
    "        print(f\"  ✗ {model_name} results not found\")\n",
    "    \n",
    "    if predictions_file.exists():\n",
    "        model_predictions[model_name] = pd.read_csv(predictions_file)\n",
    "        print(f\"  ✓ Loaded {model_name} predictions\")\n",
    "\n",
    "print(f\"\\nLoaded results for {len(model_results)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "if model_results:\n",
    "    # Extract test set results for comparison\n",
    "    test_results = {}\n",
    "    for model_name, results in model_results.items():\n",
    "        if 'test' in results:\n",
    "            test_results[model_name] = results['test']\n",
    "    \n",
    "    if test_results:\n",
    "        comparison_df = compare_models(test_results)\n",
    "        print(\"Model Performance Comparison (Test Set):\")\n",
    "        print(\"=\" * 50)\n",
    "        print(comparison_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No test results found for comparison\")\n",
    "else:\n",
    "    print(\"No model results loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Model Performance\n",
    "\n",
    "Create comprehensive visualizations comparing model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison plots\n",
    "if len(test_results) >= 2:\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # AUROC comparison\n",
    "    models = list(test_results.keys())\n",
    "    aurocs = [test_results[model]['auroc']['mean'] for model in models]\n",
    "    auroc_cis = [(test_results[model]['auroc']['ci_lower'], \n",
    "                  test_results[model]['auroc']['ci_upper']) for model in models]\n",
    "    \n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(models)))\n",
    "    bars1 = ax1.bar(models, aurocs, color=colors, alpha=0.7)\n",
    "    \n",
    "    # Add error bars\n",
    "    ci_lower = [aurocs[i] - auroc_cis[i][0] for i in range(len(models))]\n",
    "    ci_upper = [auroc_cis[i][1] - aurocs[i] for i in range(len(models))]\n",
    "    ax1.errorbar(models, aurocs, yerr=[ci_lower, ci_upper], fmt='none', color='black', capsize=5)\n",
    "    \n",
    "    ax1.set_ylabel('AUROC')\n",
    "    ax1.set_title('Model AUROC Comparison')\n",
    "    ax1.set_ylim(0.5, 1.0)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, auroc in zip(bars1, aurocs):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{auroc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # AUPRC comparison\n",
    "    auprcs = [test_results[model]['auprc']['mean'] for model in models]\n",
    "    auprc_cis = [(test_results[model]['auprc']['ci_lower'], \n",
    "                  test_results[model]['auprc']['ci_upper']) for model in models]\n",
    "    \n",
    "    bars2 = ax2.bar(models, auprcs, color=colors, alpha=0.7)\n",
    "    \n",
    "    ci_lower = [auprcs[i] - auprc_cis[i][0] for i in range(len(models))]\n",
    "    ci_upper = [auprc_cis[i][1] - auprcs[i] for i in range(len(models))]\n",
    "    ax2.errorbar(models, auprcs, yerr=[ci_lower, ci_upper], fmt='none', color='black', capsize=5)\n",
    "    \n",
    "    ax2.set_ylabel('AUPRC')\n",
    "    ax2.set_title('Model AUPRC Comparison')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, auprc in zip(bars2, auprcs):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{auprc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Calibration comparison (Brier Score)\n",
    "    brier_scores = [test_results[model]['calibration']['brier_score'] for model in models]\n",
    "    bars3 = ax3.bar(models, brier_scores, color=colors, alpha=0.7)\n",
    "    ax3.set_ylabel('Brier Score')\n",
    "    ax3.set_title('Calibration Quality (Brier Score)')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, brier in zip(bars3, brier_scores):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{brier:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Clinical utility (Sensitivity at 90% specificity)\n",
    "    sens_90 = [test_results[model]['sens_at_spec90']['sensitivity'] for model in models]\n",
    "    bars4 = ax4.bar(models, sens_90, color=colors, alpha=0.7)\n",
    "    ax4.set_ylabel('Sensitivity')\n",
    "    ax4.set_title('Sensitivity at 90% Specificity')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, sens in zip(bars4, sens_90):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{sens:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.suptitle('Comprehensive Model Performance Comparison', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient models for comparison plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ROC and Precision-Recall Curves\n",
    "\n",
    "Compare ROC and PR curves across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and PR curves comparison\n",
    "if model_predictions:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(model_predictions)))\n",
    "    \n",
    "    for i, (model_name, pred_df) in enumerate(model_predictions.items()):\n",
    "        y_true = pred_df['true_label'].values\n",
    "        y_score = pred_df['predicted_prob'].values\n",
    "        \n",
    "        # ROC curve\n",
    "        from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        ax1.plot(fpr, tpr, color=colors[i], linewidth=2,\n",
    "                label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "        \n",
    "        # PR curve\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        \n",
    "        ax2.plot(recall, precision, color=colors[i], linewidth=2,\n",
    "                label=f'{model_name} (AUC = {pr_auc:.3f})')\n",
    "    \n",
    "    # ROC plot formatting\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title('ROC Curves Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # PR plot formatting\n",
    "    baseline = np.mean(y_true)\n",
    "    ax2.axhline(y=baseline, color='k', linestyle='--', alpha=0.5, \n",
    "               label=f'Baseline ({baseline:.3f})')\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_title('Precision-Recall Curves Comparison')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No prediction data available for curve plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Comparisons\n",
    "\n",
    "Perform DeLong tests to compare model performance statistically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeLong statistical comparisons\n",
    "if len(model_predictions) >= 2:\n",
    "    print(\"Statistical Model Comparisons (DeLong Test)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    model_names = list(model_predictions.keys())\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    for i, model1 in enumerate(model_names):\n",
    "        for j, model2 in enumerate(model_names[i+1:], i+1):\n",
    "            # Get predictions\n",
    "            pred1 = model_predictions[model1]\n",
    "            pred2 = model_predictions[model2]\n",
    "            \n",
    "            # Ensure same samples\n",
    "            common_samples = set(pred1['sample_id']) & set(pred2['sample_id'])\n",
    "            \n",
    "            if len(common_samples) > 0:\n",
    "                pred1_common = pred1[pred1['sample_id'].isin(common_samples)].sort_values('sample_id')\n",
    "                pred2_common = pred2[pred2['sample_id'].isin(common_samples)].sort_values('sample_id')\n",
    "                \n",
    "                y_true = pred1_common['true_label'].values\n",
    "                y_score1 = pred1_common['predicted_prob'].values\n",
    "                y_score2 = pred2_common['predicted_prob'].values\n",
    "                \n",
    "                # DeLong test\n",
    "                delong_result = delong_test(y_true, y_score1, y_score2)\n",
    "                \n",
    "                comparison_results.append({\n",
    "                    'Model 1': model1,\n",
    "                    'Model 2': model2,\n",
    "                    'AUROC 1': delong_result['auc1'],\n",
    "                    'AUROC 2': delong_result['auc2'],\n",
    "                    'Difference': delong_result['auc_diff'],\n",
    "                    'p-value': delong_result['p_value'],\n",
    "                    'Significant': delong_result['significant']\n",
    "                })\n",
    "                \n",
    "                significance = \"***\" if delong_result['p_value'] < 0.001 else \\\n",
    "                              \"**\" if delong_result['p_value'] < 0.01 else \\\n",
    "                              \"*\" if delong_result['p_value'] < 0.05 else \"ns\"\n",
    "                \n",
    "                print(f\"{model1} vs {model2}:\")\n",
    "                print(f\"  AUROC difference: {delong_result['auc_diff']:+.4f}\")\n",
    "                print(f\"  p-value: {delong_result['p_value']:.4f} ({significance})\")\n",
    "                print()\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    if comparison_results:\n",
    "        comparison_df = pd.DataFrame(comparison_results)\n",
    "        print(\"\\nComparison Summary:\")\n",
    "        print(comparison_df.round(4))\n",
    "else:\n",
    "    print(\"Need at least 2 models for statistical comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Interpretation\n",
    "\n",
    "Extract and visualize feature importance for interpretable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis (for baseline models)\n",
    "print(\"Feature Importance Analysis\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Load feature data to get feature names\n",
    "sys.path.append('../src')\n",
    "from cfdna.features import prepare_features\n",
    "\n",
    "try:\n",
    "    # Load configuration\n",
    "    import yaml\n",
    "    with open('../data/synthetic_config.yaml') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Prepare features to get feature names\n",
    "    data = prepare_features(Path('../data'), config)\n",
    "    feature_names = data['X'].columns.tolist()\n",
    "    \n",
    "    # For demonstration, create mock feature importance scores\n",
    "    # In practice, these would be extracted from trained models\n",
    "    \n",
    "    # Mock importance scores (would normally load from model artifacts)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Simulate methylation features being more important\n",
    "    meth_features = [f for f in feature_names if f.startswith('dmr_')]\n",
    "    frag_features = [f for f in feature_names if not f.startswith('dmr_')]\n",
    "    \n",
    "    importance_scores = {}\n",
    "    \n",
    "    # Higher importance for methylation features\n",
    "    for feature in meth_features:\n",
    "        importance_scores[feature] = np.random.exponential(0.5)\n",
    "    \n",
    "    # Lower importance for fragmentomics features\n",
    "    for feature in frag_features:\n",
    "        importance_scores[feature] = np.random.exponential(0.2)\n",
    "    \n",
    "    # Sort and get top features\n",
    "    sorted_features = sorted(importance_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_features = sorted_features[:15]\n",
    "    \n",
    "    print(f\"Top 15 Most Important Features:\")\n",
    "    for i, (feature, score) in enumerate(top_features, 1):\n",
    "        feature_type = \"Methylation\" if feature.startswith('dmr_') else \"Fragmentomics\"\n",
    "        print(f\"  {i:2d}. {feature:<20} ({feature_type:<13}): {score:.3f}\")\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Top features bar plot\n",
    "    features, scores = zip(*top_features)\n",
    "    colors = ['lightcoral' if f.startswith('dmr_') else 'lightblue' for f in features]\n",
    "    \n",
    "    y_pos = np.arange(len(features))\n",
    "    ax1.barh(y_pos, scores, color=colors)\n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(features)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_xlabel('Importance Score')\n",
    "    ax1.set_title('Top 15 Feature Importances')\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Feature type comparison\n",
    "    meth_scores = [score for feature, score in importance_scores.items() if feature.startswith('dmr_')]\n",
    "    frag_scores = [score for feature, score in importance_scores.items() if not feature.startswith('dmr_')]\n",
    "    \n",
    "    ax2.boxplot([meth_scores, frag_scores], labels=['Methylation', 'Fragmentomics'])\n",
    "    ax2.set_ylabel('Importance Score')\n",
    "    ax2.set_title('Feature Importance by Type')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nFeature Type Summary:\")\n",
    "    print(f\"  Methylation features: {len(meth_scores)} (mean importance: {np.mean(meth_scores):.3f})\")\n",
    "    print(f\"  Fragmentomics features: {len(frag_scores)} (mean importance: {np.mean(frag_scores):.3f})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in feature importance analysis: {e}\")\n",
    "    print(\"This would normally load actual feature importance from trained models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Evaluation Report\n",
    "\n",
    "Use the CLI to generate a comprehensive evaluation report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate evaluation report using CLI\n",
    "print(\"Generating comprehensive evaluation report...\")\n",
    "\n",
    "report_path = artifacts_dir / \"evaluation_report.md\"\n",
    "\n",
    "cmd = [\n",
    "    'python', '-m', 'cfdna.eval',\n",
    "    str(artifacts_dir),\n",
    "    str(report_path),\n",
    "    '--models'] + models_to_train\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True, cwd='../src')\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"✓ Evaluation report generated successfully\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "    # Display part of the report\n",
    "    if report_path.exists():\n",
    "        with open(report_path) as f:\n",
    "            report_content = f.read()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EVALUATION REPORT PREVIEW\")\n",
    "        print(\"=\"*60)\n",
    "        # Show first 1000 characters\n",
    "        print(report_content[:1000] + \"...\" if len(report_content) > 1000 else report_content)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Full report saved to: {report_path}\")\n",
    "        print(\"=\"*60)\nelse:\n",
    "    print(\"✗ Report generation failed:\")\n",
    "    print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Model Performance**: Compare AUROC and AUPRC across different approaches\n",
    "2. **Statistical Significance**: DeLong tests show whether performance differences are significant\n",
    "3. **Calibration Quality**: Brier scores indicate how well-calibrated the probability predictions are\n",
    "4. **Clinical Utility**: Sensitivity at high specificity shows practical clinical value\n",
    "5. **Feature Importance**: Methylation vs fragmentomics contribution to predictions\n",
    "\n",
    "### Model Selection Criteria:\n",
    "\n",
    "- **Best Overall Performance**: Highest AUROC with tight confidence intervals\n",
    "- **Clinical Utility**: High sensitivity at 90-95% specificity\n",
    "- **Calibration**: Low Brier score for reliable probability estimates\n",
    "- **Interpretability**: Feature importance aligns with biological expectations\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Hyperparameter Optimization**: Further tune the best-performing model\n",
    "2. **Ensemble Methods**: Combine complementary models\n",
    "3. **External Validation**: Test on independent datasets\n",
    "4. **Clinical Translation**: Develop decision support tools\n",
    "5. **Regulatory Preparation**: Document for FDA submission\n",
    "\n",
    "**Next notebook**: `04_stats_and_calibration.ipynb` - Deep dive into statistical validation and calibration analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}